version: '3.8'

services:
  namenode:
    image: apache/hadoop:3.4.1
    hostname: namenode
    user: root
    environment:
      - HADOOP_HOME=/opt/hadoop
#      - HDFS_NAMENODE_OPTS=-Xmx4g -XX:+UseG1GC
    volumes:
      - namenode_data:/opt/hadoop/data/nameNode
    configs:
      - source: core_site_config
        target: /opt/hadoop/etc/hadoop/core-site.xml
      - source: hdfs_site_config
        target: /opt/hadoop/etc/hadoop/hdfs-site.xml
      - source: start_hdfs_script
        target: /start-hdfs.sh
        mode: 0755
    ports:
      - "9870:9870"
      - "9000:9000"
    command: ["/bin/bash", "/start-hdfs.sh"]
    deploy:
      mode: replicated 
      replicas: 1
      placement:
        constraints:
          - node.role == manager
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9870"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    networks:
      - hadoop_spark_net

  datanode-high:
    image: apache/hadoop:3.4.1
    hostname: "datanode-high-{{.Node.Hostname}}"
    user: root
    environment:
      - HADOOP_HOME=/opt/hadoop
      - NODE_HOSTNAME={{.Node.Hostname}}
#      - HADOOP_DATANODE_OPTS=-Xmx6g -XX:+UseG1GC # Consider tuning based on high-perf node RAM
      - NODE_PROFILE=high
    volumes:
      - type: bind
        source: /data/hadoop # Ensure this path exists on target nodes
        target: /opt/hadoop/data/dataNode
    configs:
      - source: core_site_config
        target: /opt/hadoop/etc/hadoop/core-site.xml
      - source: hdfs_site_config
        target: /opt/hadoop/etc/hadoop/hdfs-site.xml
      - source: datanode_script
        target: /init-datanode.sh
        mode: 0755
    depends_on:
      - namenode
    command: ["/bin/bash", "/init-datanode.sh"]
    deploy:
      mode: replicated
      replicas: 2 # For the 2 high-performance nodes
      placement:
        constraints:
          - node.labels.h-role == worker
          - node.labels.performance == high
    networks:
      - hadoop_spark_net

  datanode-mid:
    image: apache/hadoop:3.4.1
    hostname: "datanode-mid-{{.Node.Hostname}}"
    user: root
    environment:
      - HADOOP_HOME=/opt/hadoop
      - NODE_HOSTNAME={{.Node.Hostname}}
#      - HADOOP_DATANODE_OPTS=-Xmx4g -XX:+UseG1GC # Consider tuning based on mid-perf node RAM
      - NODE_PROFILE=mid
    volumes:
      - type: bind
        source: /data/hadoop # Ensure this path exists on target nodes
        target: /opt/hadoop/data/dataNode
    configs:
      - source: core_site_config
        target: /opt/hadoop/etc/hadoop/core-site.xml
      - source: hdfs_site_config
        target: /opt/hadoop/etc/hadoop/hdfs-site.xml
      - source: datanode_script
        target: /init-datanode.sh
        mode: 0755
    depends_on:
      - namenode
    command: ["/bin/bash", "/init-datanode.sh"]
    deploy:
      mode: replicated
      replicas: 1 # For the 1 mid-performance node
      placement:
        constraints:
          - node.labels.h-role == worker
          - node.labels.performance == mid
    networks:
      - hadoop_spark_net

  datanode-low:
    image: apache/hadoop:3.4.1
    hostname: "datanode-low-{{.Node.Hostname}}"
    user: root
    environment:
      - HADOOP_HOME=/opt/hadoop
      - NODE_HOSTNAME={{.Node.Hostname}}
#      - HADOOP_DATANODE_OPTS=-Xmx2g -XX:+UseG1GC # Consider tuning based on low-perf node RAM
      - NODE_PROFILE=low
    volumes:
      - type: bind
        source: /data/hadoop # Ensure this path exists on target nodes
        target: /opt/hadoop/data/dataNode
    configs:
      - source: core_site_config
        target: /opt/hadoop/etc/hadoop/core-site.xml
      - source: hdfs_site_config
        target: /opt/hadoop/etc/hadoop/hdfs-site.xml
      - source: datanode_script
        target: /init-datanode.sh
        mode: 0755
    depends_on:
      - namenode
    command: ["/bin/bash", "/init-datanode.sh"]
    deploy:
      mode: replicated
      replicas: 1 # For the 1 low-performance node
      placement:
        constraints:
          - node.labels.h-role == worker
          - node.labels.performance == low
    networks:
      - hadoop_spark_net

  spark-master:
    image: bitnami/spark:3.5.5
    hostname: spark-master
    user: root
    environment:
      - SPARK_MODE=master
      - SPARK_MASTER_HOST=spark-master
      - SPARK_LOCAL_IP=0.0.0.0
      - HADOOP_CONF_DIR=/opt/hadoop/etc/hadoop
      - SPARK_DRIVER_HOST=spark-master
      - SPARK_DRIVER_BIND_ADDRESS=0.0.0.0
      - SPARK_USER=root  # Tell Spark to use root identity
    ports:
      - "8080:8080"
      - "7077:7077"
    configs:
      - source: core_site_config
        target: /opt/hadoop/etc/hadoop/core-site.xml
      - source: hdfs_site_config
        target: /opt/hadoop/etc/hadoop/hdfs-site.xml
    deploy:
      mode: replicated 
      replicas: 1      
      placement:
        constraints:
          - node.labels.spark-role == master
    networks:
      - hadoop_spark_net

  spark-worker-high:
    image: bitnami/spark:3.5.5
    hostname: "spark-worker-high-{{.Node.Hostname}}"
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - HADOOP_CONF_DIR=/opt/hadoop/etc/hadoop
      - SPARK_WORKER_WEBUI_PORT=8081
      - SPARK_WORKER_CORES=10
      - SPARK_WORKER_MEMORY=12G
    ports:
      - "8081:8081" # This port will be mapped on each node running a high-perf worker
    configs:
      - source: core_site_config
        target: /opt/hadoop/etc/hadoop/core-site.xml
      - source: hdfs_site_config
        target: /opt/hadoop/etc/hadoop/hdfs-site.xml
    deploy:
      mode: replicated
      replicas: 2 # For the 2 high-performance nodes
      placement:
        constraints:
          - node.labels.spark-role == worker
          - node.labels.performance == high
    networks:
      - hadoop_spark_net

  spark-worker-mid:
    image: bitnami/spark:3.5.5
    hostname: "spark-worker-mid-{{.Node.Hostname}}"
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - HADOOP_CONF_DIR=/opt/hadoop/etc/hadoop
      - SPARK_WORKER_WEBUI_PORT=8082 # Using a different port for potential UI access if needed, though typically not accessed directly per worker often.
      - SPARK_WORKER_CORES=6
      - SPARK_WORKER_MEMORY=5G
    ports:
      - "8082:8082" # Exposing on a different host port to avoid conflict if multiple worker types were on same node (not the case here due to placement)
    configs:
      - source: core_site_config
        target: /opt/hadoop/etc/hadoop/core-site.xml
      - source: hdfs_site_config
        target: /opt/hadoop/etc/hadoop/hdfs-site.xml
    deploy:
      mode: replicated
      replicas: 1 # For the 1 mid-performance node
      placement:
        constraints:
          - node.labels.spark-role == worker
          - node.labels.performance == mid
    networks:
      - hadoop_spark_net

  jupyter-notebook:
    image: jupyter/pyspark-notebook:latest 
    hostname: jupyter-notebook
    environment:
      - JUPYTER_ENABLE_LAB=yes
      - SPARK_HOME=/usr/local/spark
      - HADOOP_CONF_DIR=/opt/hadoop/etc/hadoop
      - JUPYTER_TOKEN=sparkcluster
    ports:
      - "8888:8888"
    volumes:
      - .:/home/jovyan/work
    configs:
      - source: core_site_config
        target: /opt/hadoop/etc/hadoop/core-site.xml
      - source: hdfs_site_config
        target: /opt/hadoop/etc/hadoop/hdfs-site.xml
    deploy:
      mode: replicated 
      replicas: 1      
      placement:
        constraints:
          - node.role == manager 
    networks:
      - hadoop_spark_net

networks:
  hadoop_spark_net:
    external: true
  
volumes:
  namenode_data:
    driver: local

configs:
  core_site_config:
    file: ./hadoop-config/core-site.xml
  hdfs_site_config:
    file: ./hadoop-config/hdfs-site.xml
  start_hdfs_script:
    file: ./start-hdfs.sh
  datanode_script:
    file: ./init-datanode.sh